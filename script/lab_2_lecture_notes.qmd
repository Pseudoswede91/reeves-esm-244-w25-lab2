---
title: "Lab 2 Lecture Notes"
author: "Reeves Erickson"
format: 
  html:
    code-folding: show
    embed-resources: true
execute:
  warning: false
  message: false
theme: darkly
---
"Execute warning/message false" makes it so that when we knit, the messages don't pop up when we run ex. library(tidyverse)




```{r}
# load libraries
library(tidyverse)
library(palmerpenguins)
```

What does the following code chunk do? Why do we want to do these steps?

```{r}
penguins_clean<-penguins |> 
  drop_na() |> 
  rename(mass=body_mass_g,
         bill_l=bill_length_mm,
         bill_d=bill_depth_mm,
         flip_l=flipper_length_mm)
```

## Part 1: Set up models

We are tasked with providing a penguin growth model to support conservation efforts in Antartica. The lead researcher needs an accurate, but parsimonious model to predict penguin body mass based on observed characteristics. They asked us to analyze 3 models:

- Model 1: Bill length, bill depth, flipper length, species, sex, and island

- Model 2: Bill length, bill depth, flipper length, species, and sex

- Model 3: Bill depth, flipper length, species, and sex

Run a linear model for each model specification. Summarize your findings. Use the `penguins_clean` dataframe.

**New Feature!**

R is able to recognize formulas if saved to the global environment. Take advantage of that using the following code chunk as inspiration:

```{r}
#| eval: false

#variable name
#f1   <-  dep_var~col_name_1+col_name_2+col_name_3

#mdl<-lm(f1, data=df_where_column_names_come_frome)

#1 Bill length, bill depth, flipper length, species, sex, and island
f1 <- mass~bill_l+bill_d+flip_l+species+sex+island

#m2 Bill length, bill depth, flipper length, species, and sex
f2 <- mass~bill_l+bill_d+flip_l+species+sex

#m3 Bill depth, flipper length, species, and sex
f3 <- mass~bill_l+flip_l+species+sex

mdl1<-lm(f1,data=penguins_clean)
mdl2<-lm(f2,data=penguins_clean)
mdl3<-lm(f3,data=penguins_clean)

summary(mdl1)
summary(mdl2)
summary(mdl3)
```
The outputs of these show the r squared and says that they have an 87% match (ex. r-squared is 0.8701)

### AIC

```{r}
#| eval: false
AIC(mdl1,mdl2,mdl3)
```
We can use AIC instead of R-squared, We want to choose the lowest AIC value because it's going to be the model with the fewest parameters. So model 3 is best.

Use AIC to justify your model selection. What edits do you need to make in order for the chunk below to work? Interpret the output. *Bonus:* Try to make the rendered output pretty by putting it into a table.


```{r}
#bonus
# I don't know what the story is with this. it was in the key. Nate didn't use it.
library(kableExtra)
AIC(mdl1,mdl2,mdl3) |> 
  kable() |> 
  kable_classic_2()
```

## Comparing models with Cross Validation

Now we're going to use 10-fold cross validation to help us select the models. Write out some pseudocode to outline the process we'll need to implement.

Pseudocode:
1) select how much training data we're going to use (10 folds)
2) split the data into test and training by randomly sampling
3) what metric?
  root mean squared error
  let's make a function for rmse
4) for loop 
      to apply the model to each training set
      make predictions on the test set with fitted training model
close loop:

summarize our RMSE (which model on average was best)
Final model built on whole dataset


### Accuracy criteria

What metric is going to help us identify which model performed better?

[Here's an extensive list of options](https://www.geeksforgeeks.org/metrics-for-machine-learning-model/#)

We'll use root mean square error to start as it is the most widely used.

What is the difference between these two functions? Create two example vectors `x` and `y` to test each. Make sure to run the functions before trying them out. Which do you prefer using?

```{r}
calc_rmse<-function(x,y){
  rmse <- (x-y)^2 |> 
    mean() |> 
    sqrt()
  return(rmse)
}
print(calc_rmse)


calc_rmse_2<-function(x,y){
  rmse<- sqrt(mean((x-y)^2))
  
  return(rmse)
}

x<-rnorm(10000)
y<-rnorm(10000)

calc_rmse(x,y)
calc_rmse_2(x,y)
```
The functions do the same thing. question is how do you want to type it out?
in the first one we are putting the functions in the curly brackets and the steps are going to be what we need them to be. we defined the function, told it to take x aned y, square it , take the mean, take the sqrt of that, and conclude. This first function is easier to read and see what's happening. the shorter one is shorter to type. point is, we choose how we write our functions and there is flexibility in doing so.


The first function is cleaner and makes more sense what it does. We'll use that moving forward


### Testing and Training split

We need to randomly assign every data point to a fold. We're going to want 10 folds. 

**New Function!**

`sample()` takes a random draw from a vector we pass into it. For example, we can tell sample to extract a random value from a vector of 1 through 5

```{r}
#| eval: false
#| 
ex<-seq(1,5)
sample(ex,size=1)

# we can create a random sample of any size with the size term.

# Why doesn't the first line work while the second does?
sample(ex,size=10)
sample(ex,size=10,replace=TRUE)

#Describe in words the replace argument.
#It's like taking scrabble letters out of a bag. In sample(ex,size=10), you're trying to take 10 scrabble letters out of a bag with 5 tiles. In sample(ex,size=10,replace=TRUE), you're taking a tile out of the bag, putting it back in the bag, and pulling another tile a total of 10 times.

```
Think about the implications of this. If we're running a model, we could get different results every time.

*Sample uses random numbers to extract the information. Everyones computers uses different pseduo-random number generator based on the internal clock cycling. We can get a random order, but keep it the same random stream of numbers with the set.seed function*

```{r}
#seed

set.seed(42)
sample(ex,size=10,replace=TRUE)
```


Now let's use sample in tidyverse structure to group the data into different folds.

```{r}
folds<-10

fold_vec<-rep(1:folds,length.out=nrow(penguins_clean))

penguins_fold<-penguins_clean |> mutate(group=sample(fold_vec,size=n(),replace=FALSE))
  

#check to make sure the fold groups are balanced

table(penguins_fold$group)
```
Results of table is balanced samples. they're evenly divided number of observations into 10 groups.


Create dataframes called `test_df` and `train_df` that split the penguins data into a train or test sample

```{r}
# datasets here

test_df<-penguins_fold |> 
  filter(group==1)

train_df<-penguins_fold |> 
  filter(group!= 1)
#!= if it's not equal to 1, give me everything else
```


Now fit each model to the training set using the `lm()`. Name each model `training_lmX` where X is the number of the formula.
```{r}
training_lm1<-lm(f1,data=train_df)
training_lm1<-lm(f2,data=train_df)
training_lm1<-lm(f3,data=train_df)
```

**New Function!**

`predict()` uses R models to run predictions with new data. In our case the structure would look something like what we see below. What do I need to do to make this chunk work?

```{r}
predict_test<-test_df |> 
  mutate(model1 = predict(training_lm1,test_df),
         model2 = predict(training_lm2,test_df),
         model3 = predict(training_lm3,test_df))
```


Calculate the RMSE of the first fold test predictions. Hint: Use summarize to condense the `predict_test` dataframe.

```{r}
rmse_predict_test<-predict_test %>% 
  summarize()
```





```{r}
rmse_predict_test<-predict_test |> 
  summarize(rmse_mdl1=calc_rmse(model1,mass),
            rmse_mdl2=calc_rmse(model2,mass),
            rmse_mdl3=calc_rmse(model3,mass))

rmse_predict_test
```

What are the results just looking at the first fold?

### 10-fold CV: For Loop

Our general structure works with one fold. Now we need to evaluate across all 10 folds for each model. Let's use a for loop to iterate over the folds for just one model first.

```{r}

### initialize a blank vector
rmse_vec<-vector(mode='numeric',length=folds)  #Why?

for( i in 1:folds){

  # separate into test and train
    kfold_test_df <- penguins_fold %>%
    filter(group == i)
    
  kfold_train_df <- penguins_fold %>%
    filter(group != i)
  
  # Run for one model
  
  kfold_lm1 <- lm(f1, data = kfold_train_df)
  
  #Get the predictions from the model
  
  kfold_pred_df <- kfold_test_df %>%
    mutate(mdl = predict(kfold_lm1, kfold_test_df))
  
  # Summarize/calculate the rmse for that model
   kfold_rmse <- kfold_pred_df %>%
    summarize(rmse_mdl = calc_rmse(mdl, mass))
  
  rmse_vec[i]<-kfold_rmse$rmse_mdl
}

# Average value for the first model
mean(rmse_vec)
```


Great we made a for loop for one model. Now we would have to do it again and again for the other formulas. To reduce copy/pasting let's make the innerpart of the for loop into a function. I gave you the starting pieces below. Complete the rest of the function

```{r}
kfold_cv<-function(i,df,formula){

  #split into test and train
  
  kfold_train_df <- df %>%
    filter(group != i)

  kfold_test_df <- df %>%
    filter(group == i)
  
  # run model

  kfold_lm <- lm(formula, data = kfold_train_df)

  # get predictions on test set
  kfold_pred_df <- kfold_test_df %>%
    mutate(mdl = predict(kfold_lm, kfold_test_df))
  
  # calculate RMSE
  kfold_rmse <- kfold_pred_df %>%
    summarize(rmse_mdl = calc_rmse(mdl, mass))


  return(kfold_rmse$rmse_mdl)
  
  
}
```





### 10-fold CV: Purrr

Since we already defined the function that does CV for each model. We can use purrr to easily get all the results and store it in a dataframe.

```{r}
rmse_df<-data.frame(j=1:folds) |> mutate(rmse_mdl1 = map_dbl(j, kfold_cv, df=penguins_fold,formula=f1),
                                         rmse_mdl2=map_dbl(j,kfold_cv,df=penguins_fold,formula=f2),
                                         rmse_mdl3=map_dbl(j,kfold_cv,df=penguins_fold,formula=f3))

rmse_means<-rmse_df |> 
  summarize(across(starts_with('rmse'),mean))
```


## Final Model Selection

Between AIC and the RMSE scores of the Cross Validation, which model does the best job of predicting penguin bodymass?


The final step is to run the selected model on all the data. Fit a final model and provide the summary table.

```{r}
final_mod<-lm(f2,data=penguins_clean)
rmse_predict_test<-predict_test %>% 
  summarize(rmse_mdl1=calc_rmse(mass,model1),
          rmse_mdl2=calc_rmse(mass,model2),
          rmse_mdl3=calc_rmse(mass,model3))
```


'For' loops!
```{r}
names<-c("Reeves","Jessie","Madison","Brian","Debbie")

for(v in names){
  print(v)
}

vec<-seq(1,10)
for(k in vec){
  print(k)
}

vec<-seq(1,10)
for(k in vec){
  print(k^2)
}
```

### 10-fold CV: For Loop

Our general structure works with one fold. Now we need to evaluate across all 10 folds for each model. Let's use a for loop to iterate over the folds for just one model first.

```{r}
rmse_vec<-vector(mode='numeric', length=folds)

for( i  in 1:folds){
  # separate into test and train
    kfold_test_df <- penguins_fold %>%
    filter(group == i)
    
  kfold_train_df <- penguins_fold %>%
    filter(group != i)
  
  # Run for one model
  
  kfold_lm1 <- lm(f1, data = kfold_train_df)
  
  #Get the predictions from the model
  
  kfold_pred_df <- kfold_test_df %>%
    mutate(mdl = predict(kfold_lm1, kfold_test_df))
  
  # Summarize/calculate the rmse for that model
   kfold_rmse <- kfold_pred_df %>%
    summarize(rmse_mdl1 = calc_rmse(mdl, mass))
  
  rmse_vec[i]<-kfold_rmse$rmse_mdl1
}

# Average value for the first model
mean(rmse_vec)
```


Great we made a for loop for one model. Now we would have to do it again and again for the other formulas. To reduce copy/pasting let's make the innerpart of the for loop into a function. I gave you the starting pieces below. Complete the rest of the function

```{r}
kfold_cv<-function(i,df,formula){
  
  #split into train and test
  kfold_train_df<-df %>% 
    filter(group!=i)
  kfold_test_df<-df %>% 
    filter(group==i)
  
  #run model
  kfold_lm<-lm(formula,data=kfold_train_df)
  
  #get prediciton
  kfold_pred_df<-kfold_test_df %>% 
    mutate(mdl = predict(kfold_lm, kfold_test_df))
  
  # calculate RMSE
  kfold_rmse <- kfold_pred_df %>%
    summarize(rmse_mdl = calc_rmse(mdl, mass))


  return(kfold_rmse$rmse_mdl)
  
  
}
```



### 10-fold CV: Purrr

Since we already defined the function that does CV for each model. We can use purrr to easily get all the results and store it in a dataframe.

```{r}
rmse_df<-data.frame(j=1:folds) |> mutate(rmse_mdl1 = map_dbl(j, kfold_cv, df=penguins_fold,formula=f1),
                                         rmse_mdl2=map_dbl(j,kfold_cv,df=penguins_fold,formula=f2),
                                         rmse_mdl3=map_dbl(j,kfold_cv,df=penguins_fold,formula=f3))

rmse_means<-rmse_df |> 
  summarize(across(starts_with('rmse'),mean))
```


## Final Model Selection

Between AIC and the RMSE scores of the Cross Validation, which model does the best job of predicting penguin bodymass?
RMSE scores via cross validation do the best job of predicting.

The final step is to run the selected model on all the data. Fit a final model and provide the summary table.
```{r}
final_model<-lm(f2,data=penguins_clean)

summary(final_model)
```


Render your document, commit changes, and push to github.


